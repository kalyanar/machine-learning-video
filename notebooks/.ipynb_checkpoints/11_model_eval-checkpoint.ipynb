{
 "metadata": {
  "name": "",
  "signature": "sha256:7edfc27957f6f1ef0103ccb39a3c1ab2a3005e63695579040b91c87c5f66c904"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction to model evaluation (3 min)\n",
      "\n",
      "- we have our analyses and models. which one is the best? have to evaluate to find out\n",
      "- need to determine with what accuracy we're predicting/clustering/etc \n",
      "- Model evaluation as part of the machine learning process\n",
      "\n",
      "# Model evaluation methods (13 min)\n",
      "\n",
      "- Recap the methods already discussed. \n",
      "    - cross val\n",
      "    - silhouette score\n",
      "    - score method on models\n",
      "    - explained variance for PCA\n",
      "- Other methods\n",
      "    - score: compares the prediction on the data to the actual labels/target values and calculates how accurate the model is. Like cross validation except happens once."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "digits = load_digits()\n",
      "tree = RandomForestClassifier()\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
      "\n",
      "tree.fit(x_train, y_train)\n",
      "predicted = tree.predict(x_test)\n",
      "tree.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.92666666666666664"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "confusion matrix: visually show where the algorithm is confused. of the total in the row, how many were classified incorrectly?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "confusion_matrix(y_test, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([[40,  0,  0,  0,  2,  0,  0,  0,  0,  0],\n",
        "       [ 0, 36,  2,  0,  0,  0,  0,  0,  1,  0],\n",
        "       [ 0,  0, 35,  1,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0, 46,  0,  1,  0,  0,  0,  0],\n",
        "       [ 0,  1,  0,  0, 41,  0,  0,  0,  0,  0],\n",
        "       [ 0,  1,  0,  1,  1, 49,  0,  0,  1,  2],\n",
        "       [ 0,  0,  0,  0,  0,  0, 41,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  1,  1,  0, 48,  0,  0],\n",
        "       [ 0,  6,  0,  1,  0,  0,  1,  0, 35,  0],\n",
        "       [ 0,  1,  0,  4,  1,  1,  0,  1,  1, 46]])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- precision: the ratio true positivites / (true positives + false positives)\n",
      "- the ability of the classifier not to label as positive a sample that is negative"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import precision_score\n",
      "\n",
      "precision_score(y_test, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "0.93027824538140824"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- recall: true positivites / (true positives + false negatives)\n",
      "- the ability of the classifier to find all the positive samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import recall_score\n",
      "\n",
      "recall_score(y_test, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.92666666666666664"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- classification_report: report showing the main classification metrics\n",
      "- f1 score: weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0\n",
      "- support: number of occurrences of each class in the actual labels/target values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "\n",
      "print classification_report(y_test, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      0.95      0.98        42\n",
        "          1       0.80      0.92      0.86        39\n",
        "          2       0.95      0.97      0.96        36\n",
        "          3       0.87      0.98      0.92        47\n",
        "          4       0.89      0.98      0.93        42\n",
        "          5       0.94      0.89      0.92        55\n",
        "          6       0.98      1.00      0.99        41\n",
        "          7       0.98      0.96      0.97        50\n",
        "          8       0.92      0.81      0.86        43\n",
        "          9       0.96      0.84      0.89        55\n",
        "\n",
        "avg / total       0.93      0.93      0.93       450\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- explained_variance_score: explained variation measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n",
      "- The best possible score is 1.0, lower values are worse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import explained_variance_score\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "boston = load_boston()\n",
      "lr = LinearRegression()\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target)\n",
      "\n",
      "lr.fit(x_train, y_train)\n",
      "lr_predicted = lr.predict(x_test)\n",
      "\n",
      "explained_variance_score(y_test, lr_predicted) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.75964684722150266"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}