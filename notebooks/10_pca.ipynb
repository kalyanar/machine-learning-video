{
 "metadata": {
  "name": "",
  "signature": "sha256:b014cf626023302ed5585a9317eb745d9da372c134cc02b668083c8fe885c81b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction to principal component analysis (7 min)\n",
      "\n",
      "- a statistical method that reduces the dimensions of a dataset by transforming the dataset into a set of linearly uncorrelated variables\n",
      "    - set of successive components that explain a maximum amount of the variance in the dataset\n",
      "- PCA is implemented as a transformer object that learns n components in its fit method, and can be used on new data to project it on these components\n",
      "- helps w/ curse of dimensionality\n",
      "- The curse of dimensionality - too many dimensions can affect the accuracy of your models. can make your dataset noisy. not all features are needed.\n",
      "- defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible)\n",
      "- each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components\n",
      "- use on dataset, and then possibly classify or cluster\n",
      "- When do you want to use PCA?\n",
      "    - high dimensional data\n",
      "\n",
      "# Example of PCA (12 min)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "digits = load_digits()\n",
      "\n",
      "pca = PCA()\n",
      "# fit_transform: Fit the model with X and apply the dimensionality reduction on X.\n",
      "# components_ = Components with maximum variance.\n",
      "# n_components_ = estimated number of components\n",
      "pca_fit = pca.fit_transform(digits.data)\n",
      "pca_fit.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([[  1.97205455e-19,   1.73094651e-02,   2.23428835e-01, ...,\n",
        "          8.94184677e-02,   3.65977111e-02,   1.14684954e-02],\n",
        "       [ -3.64228224e-18,   1.01064569e-02,   4.90849204e-02, ...,\n",
        "         -1.76697117e-01,  -1.94547053e-02,   6.69693895e-03],\n",
        "       [  1.87065065e-18,  -1.83420720e-02,  -1.26475543e-01, ...,\n",
        "         -2.32084163e-01,  -1.67026563e-01,  -3.48043832e-02],\n",
        "       ..., \n",
        "       [  0.00000000e+00,   1.24900090e-16,  -2.00125096e-17, ...,\n",
        "          1.94289029e-16,  -2.22044605e-16,   1.97758476e-16],\n",
        "       [  0.00000000e+00,  -8.32667268e-16,  -1.85772389e-16, ...,\n",
        "          0.00000000e+00,   1.11022302e-16,  -4.44089210e-16],\n",
        "       [ -1.00000000e+00,   1.87758891e-17,  -6.37042612e-18, ...,\n",
        "         -9.62923666e-18,   1.75128847e-17,  -4.52287685e-18]])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tuning parameters (7 min)\n",
      "\n",
      "- n_components: number of components to keep. defaults to all"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# play around with different n_component values\n",
      "target_names = digits.target_names\n",
      "\n",
      "pca_tuned = PCA(n_components=10)\n",
      "pca_tuned.fit_transform(digits.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([[  1.25946645, -21.27488348,   9.46305462, ...,  -2.55347036,\n",
        "          0.58184214,  -3.62569695],\n",
        "       [ -7.9576113 ,  20.76869896,  -4.43950604, ...,   4.61593641,\n",
        "         -3.58745013,   1.07470456],\n",
        "       [ -6.99192297,   9.95598641,  -2.95855808, ...,  16.41519983,\n",
        "         -0.7192329 ,  -4.25580548],\n",
        "       ..., \n",
        "       [-10.8012837 ,   6.96025223,  -5.59955453, ...,   7.4150719 ,\n",
        "          3.96158533,  13.06509519],\n",
        "       [  4.87210009, -12.42395362,  10.17086635, ...,   4.35789156,\n",
        "         -3.93924173,  13.14525475],\n",
        "       [  0.34438963,  -6.36554919, -10.77370849, ...,  -0.66902071,\n",
        "          4.11316505,  12.56200443]])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Evaluation of PCA models (6 min)\n",
      "\n",
      "- explained_variance_ratio attribute: Percentage of variance explained by each of the selected components. sum of explained variances is equal to 1.0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([  1.48905936e-01,   1.36187712e-01,   1.17945938e-01,\n",
        "         8.40997942e-02,   5.78241466e-02,   4.91691032e-02,\n",
        "         4.31598701e-02,   3.66137258e-02,   3.35324810e-02,\n",
        "         3.07880621e-02,   2.37234084e-02,   2.27269657e-02,\n",
        "         1.82186331e-02,   1.77385494e-02,   1.46710109e-02,\n",
        "         1.40971560e-02,   1.31858920e-02,   1.24813782e-02,\n",
        "         1.01771796e-02,   9.05617439e-03,   8.89538461e-03,\n",
        "         7.97123157e-03,   7.67493255e-03,   7.22903569e-03,\n",
        "         6.95888851e-03,   5.96081458e-03,   5.75614688e-03,\n",
        "         5.15157582e-03,   4.89539777e-03,   4.28887968e-03,\n",
        "         3.73606048e-03,   3.53274223e-03,   3.36683986e-03,\n",
        "         3.28029851e-03,   3.08320884e-03,   2.93778629e-03,\n",
        "         2.56588609e-03,   2.27742397e-03,   2.22277922e-03,\n",
        "         2.11430393e-03,   1.89909062e-03,   1.58652907e-03,\n",
        "         1.51159934e-03,   1.40578764e-03,   1.16622290e-03,\n",
        "         1.07492521e-03,   9.64053065e-04,   7.74630271e-04,\n",
        "         5.57211553e-04,   4.04330693e-04,   2.09916327e-04,\n",
        "         8.24797098e-05,   5.25149980e-05,   5.05243719e-05,\n",
        "         3.29961363e-05,   1.24365445e-05,   7.04827911e-06,\n",
        "         3.01432139e-06,   1.06230800e-06,   5.50074587e-07,\n",
        "         3.42905702e-07,   1.17368844e-33,   1.17368844e-33,\n",
        "         1.15577716e-33])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca_tuned.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([ 0.14890594,  0.13618771,  0.11794594,  0.08409979,  0.05782415,\n",
        "        0.0491691 ,  0.04315987,  0.03661373,  0.03353248,  0.03078806])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wrap up (1 min)\n",
      "\n",
      "In this section we learned how to reduce the dimesionality in our datasets and how to do this in scikit-learn."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}