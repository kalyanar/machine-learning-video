{
 "metadata": {
  "name": "",
  "signature": "sha256:d8b709466dafd3e619ad371d3af3656ceba5cdafab45c377bc92954fdfd3af87"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction to linear regression (5 min)\n",
      "\n",
      "- an approach for exploring the relationship between continuous variables\n",
      "- often will be modeling a dependent variable y against explanatory x variables\n",
      "- to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation\n",
      "    - this means that the model tries to minimize the distance of each datapoint from the predicted line\n",
      "    - Example\n",
      "        - line - distance of points from line\n",
      "        - what LR does is try to minimize the distance of the points from the line\n",
      "        - the best fit model minimizes that distance for all points\n",
      "- When do you want to use linear regression?\n",
      "    - Linear data\n",
      "    - Independent data\n",
      "\n",
      "# Example of linear regression (6 min)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# in this example, our dependent variable y (target variable) \n",
      "# is Median value of owner-occupied homes in $1000\n",
      "# explanatory variables are things like per capita crime rate,\n",
      "# full value property tax rate, average number of rooms per dwelling\n",
      "# etc. A full list of explanatory variables can be found by accessing\n",
      "# ['DESCR'] like we did in segment 4.\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "boston = load_boston()\n",
      "lr = LinearRegression()\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target)\n",
      "\n",
      "lr.fit(x_train, y_train)\n",
      "lr.predict(x_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Example of ridge regression (10 min)\n",
      "\n",
      "- works to minimize a penalized version of the least squares cost function using regularization\n",
      "    - regularization is introducing additional information in order to solve an ill-posed problem or to prevent overfitting, and this information is usually of the form of a penalty for complexity\n",
      "    - what this means: goal is to find a model that fits well, so if it were applied to the training set, it should predict the values (or class labels) associated with the samples in that set with a high degree of accuracy \n",
      "    - the cost function quantifies the amount by which the prediction deviates from the actual values\n",
      "- so ridge regression seeks to penalize larger coefficient values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "lr_ridge = Ridge()\n",
      "lr_ridge.fit(x_train, y_train)\n",
      "lr_ridge.predict(x_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- parameters\n",
      "    - alpha: small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates. default is 1.\n",
      "    - solver: which solver to use in computing Ridge coefficients depending on the type of data. default is auto.\n",
      "        - \u2018auto\u2019 chooses the solver automatically based on the type of data.\n",
      "        - \u2018svd\u2019 Singular Value Decomposition of X\n",
      "        - \u2018cholesky\u2019 uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n",
      "        - \u2018sparse_cg\u2019 uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than \u2018cholesky\u2019 for large-scale data (possibility to set tol and max_iter).\n",
      "        - \u2018lsqr\u2019 uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. LSQR uses an iterative method to approximate the solution. The number of iterations required to reach a certain accuracy depends strongly on the scaling of the problem"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr_ridge_param = Ridge(alpha=0.5, solver='lsqr')\n",
      "lr_ridge_param.fit(x_train, y_train)\n",
      "lr_ridge_param.predict(x_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Example of lasso regression (10 min)\n",
      "\n",
      "- linear model that estimates sparse coefficients\n",
      "- useful in some contexts due to its tendency to prefer solutions with fewer explanatory values, effectively reducing the number of variables upon which the given solution is dependent\n",
      "- also works to minimize a penalized version of the least squares cost function, using a different form of regularization\n",
      "- best for use in a high-dimensional, sparse space"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Lasso\n",
      "\n",
      "lr_lasso = Lasso()\n",
      "lr_lasso.fit(x_train, y_train)\n",
      "lr_lasso.predict(x_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- parameters\n",
      "    - alpha again\n",
      "    - normalize: normalize regressors before regression. defaults to false\n",
      "        - to normalize: adjusting values measured on different scales to a common scale"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr_lasso_param = Lasso(alpha=0.5, normalize=True)\n",
      "lr_lasso_param.fit(x_train, y_train)\n",
      "lr_lasso_param.predict(x_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Comparing models (6 min)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "cross_val_score(lr, boston.data, boston.target)\n",
      "# run cross val on all"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wrap up (1 min)\n",
      "\n",
      "In this segment, we learned about several forms of linear regression, how to implement them in scikit-learn, and how to compare the different models we created."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}