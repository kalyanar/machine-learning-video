{
 "metadata": {
  "name": "",
  "signature": "sha256:3c6c16b14a4d6b0cd0e4af9b2b9caeee4ffb29b821590b7d270fd87b250d6810"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What is unsupervised learning? (8 min)\n",
      "\n",
      "- Again, unlabeled data/ don't know what you\u2019re looking for -- no \"ground truth\"\n",
      "- There are no class values or target values that we're trying to predict\n",
      "- possible to pull out patterns and relationships within the data without knowing what exists\n",
      "- an exploratory kind of learning, and it's good for trying to find hidden structure and correlation in the data\n",
      "- you don't decide how to group data, but you let the algorithm decide\n",
      "- Things to know slide\n",
      "- can be used in conjunction with supervised learning: for example, after clustering your dataset you might decide you want to use those clusters as the ground truth for classification or regression tasks\n",
      "- Different types of unsupervised learning\n",
      "\n",
      "# What are clustering and dimensionality reduction? (12 min)\n",
      "\n",
      "- Clustering\n",
      "    - clusters data into groups so that obervations in the same group are more similar to each other than to those in other groups\n",
      "    - purpose of clustering is to determine what relationships might exist\n",
      "    - generally does this with a distance metric, like the Euclidean distance\n",
      "    - which distance function you use largely depends on the dataset you're working with and the algorithm you're using\n",
      "    - k-means, others\n",
      "        - going to discuss k-means clustering in depth, but others\n",
      "        - hierarchical clustering: based on objects being more related to nearby objects than to objects farther away, created a hierarcy of clusters\n",
      "        - mixture models: Clusters are defined as objects belonging most likely to the same distribution\n",
      "        - DBSCAN: density-based clustering where clusters are defined as areas of higher density than the remainder of the datatset\n",
      "    - We'll focus on k-means\n",
      "- Dimensionality reduction\n",
      "    - helps to solve the curse of dimensionality\n",
      "    - process of reducing the number of random variables and the number of dimensions in your data\n",
      "    - ex. hundreds of features -- likely not all of them are useful\n",
      "    - dimensionality reduction is something you can do to preprocess your data, if you know in advance that your dataset has unnecessary dimensions in it. \n",
      "    - It can also be used if the model you're trying to create is not performing well due to a high number of features\n",
      "    - Two subsets of dimensionality reduction\n",
      "        - Feature selection\n",
      "            - is the process used to select a subset of relevant features, based on certain criteria. \n",
      "            - This process can get rid of irrelevant or redundant features, can shorten training times, and can reduce overfitting. \n",
      "            - recursively compare different feature subsets to determine which produces the most accurate model\n",
      "            - done using something like a best first search or greedy forward selection\n",
      "        - we're going to be focusing on feature extraction\n",
      "            - a technique for transforming a high-dimensional dataset to one with fewer dimensions\n",
      "            - Feature extraction transforms data into a lower dimension by mapping the data into a lower dimensional space in a way that maximizes the variance\n",
      "            - Transformations exist for both linear and nonlinear data\n",
      "                - PCA\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}