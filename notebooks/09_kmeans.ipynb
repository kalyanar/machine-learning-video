{
 "metadata": {
  "name": "",
  "signature": "sha256:95d1a9f9097f31eb39f53035db255be53cbc30e898657feb7a7ba11a3d5f0d27"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction to k-means clustering (7 min)\n",
      "\n",
      "- k is the number of cluster centers\n",
      "- a method of vector quantization or the process of mapping a large set of input values to a smaller set, in this case k\n",
      "- Partition space into Voronoi cells, the name for the regions that k-means clustering creates\n",
      "- works by dividing up a set of points, where each group is represented by its centroid point, which, in k-means clustering, are the k initial clusters you choose\n",
      "- Separate samples into k groups of equal variance and thus works to minimize within-cluster sum of squares\n",
      "- Uses the Euclidean distance metric\n",
      "- Iterative refinement\n",
      "    - assignment: assign observations to the clusters they're most similar to\n",
      "    - update cluster center to mean of cluster after assignment, so the cluster center is going to move\n",
      "    - repeat until cluster centers no longer move, meaning convergence has been reached\n",
      "    - Example (on slide)\n",
      "- Advantages\n",
      "    - Scales well - Large number of samples\n",
      "    - Efficient\n",
      "    - Will always converge\n",
      "- Disadvantages\n",
      "    - Choosing the wrong k\n",
      "    - Convergence to local minimum\n",
      "- When to use\n",
      "    - Normally distributed data\n",
      "    - Large number of samples\n",
      "    - Distance can be measured in a linear fashion\n",
      "    - non-normally distributed can skew\n",
      "    - data that can\u2019t be separated linearly may require another method (perhaps some kernel tricks)\n",
      "\n",
      "# Example of kmeans (8 min)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "iris = load_iris()\n",
      "kmeans = KMeans()\n",
      "\n",
      "# _predict will give the cluster index of each instance\n",
      "# _transform will give the distance of each instance to each cluster center\n",
      "kmeans.fit(iris.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How to choose k (7 min)\n",
      "\n",
      "- so, as you may have guessed, choosing k is the most important part of k-means clustering, bc if you do it wrong, your results won\u2019t mean much\n",
      "- graph the variance: graph the percentage of variance explained against different values of k. at some point, values of k will stop significantly explaining variance.\n",
      "- information criterion: goodness of fit of an estimated statistical model\n",
      "- cross-val: much like cross val to test how accurate your model is, you can use cross-val to test different values of k\n",
      "- Graphing the variance\n",
      "    - Unfortunately, there is no built-in function for this.\n",
      "    - We can use scipy and numpy to figure it out. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "\n",
      "import numpy as np\n",
      "from scipy.spatial.distance import cdist, pdist\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Determine your k range\n",
      "k_range = range(1,14)\n",
      "\n",
      "# Fit the kmeans model for each n_clusters = k\n",
      "k_means_var = [KMeans(n_clusters=k).fit(iris.data) for k in k_range]\n",
      "\n",
      "# Pull out the cluster centers for each model\n",
      "centroids = [X.cluster_centers_ for X in k_means_var]\n",
      "\n",
      "# Calculate the Euclidean distance from \n",
      "# each point to each cluster center\n",
      "k_euclid = [cdist(iris.data, cent, 'euclidean') for cent in centroids]\n",
      "dist = [np.min(ke,axis=1) for ke in k_euclid]\n",
      "\n",
      "# Total within-cluster sum of squares\n",
      "wcss = [sum(d**2) for d in dist]\n",
      "\n",
      "# The total sum of squares\n",
      "tss = sum(pdist(iris.data)**2)/iris.data.shape[0]\n",
      "\n",
      "# The between-cluster sum of squares\n",
      "bss = tss - wcss\n",
      "\n",
      "# elbow curve\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(k_range, bss/tss*100, 'b*-')\n",
      "ax.set_ylim((0,100))\n",
      "plt.grid(True)\n",
      "plt.xlabel('n_clusters')\n",
      "plt.ylabel('Percentage of variance explained')\n",
      "plt.title('Variance Explained vs. k')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tuning the parameters of k-means (9 min)\n",
      "\n",
      "- parameters\n",
      "    - n_clusters: that's k!\n",
      "    - max_iter: Maximum number of iterations of the k-means algorithm for a single run. defaults to 300\n",
      "    - init\n",
      "        - Method for initialization. defaults to kmeans++, which selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \n",
      "        - also \u2018random\u2019: choose k observations (rows) at random from data for the initial centroids. \n",
      "        - finally, you can pass an ndarray. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans_tuned = KMeans(n_clusters=3, max_iter=50, init='random')\n",
      "\n",
      "kmeans_tuned.fit(iris.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Evaluating our models (6 min)\n",
      "\n",
      "- Silhouette coefficient\n",
      "    - No ground truth\n",
      "    - SC: used when ground truth labels for the clusters aren\u2019t known, like we have in our data here.\n",
      "    - Mean distance between an observation and all other points in its cluster\n",
      "    - Mean distance between an observation and all other points in the next nearest cluster\n",
      "- Silhouette score in scikit-learn\n",
      "    - Mean of silhouette coefficient for all of the observations\n",
      "    - Closer to 1, the better the fit\n",
      "    - Large dataset == long time"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test 'em all\n",
      "from sklearn.metrics import silhouette_score\n",
      "\n",
      "labels = kmeans.labels_\n",
      "silhouette_score(iris.data, labels, metric='euclidean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wrap up (1 min)\n",
      "\n",
      "In this section we learned how k-means clustering works, how to choose k, and how to evaluate our model when we have no ground truth to compare it to."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}